{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7c7c9df-e048-487c-b121-1408822b81e3",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "816dde7b-7fd7-4a33-80e0-57be88cdb446",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as T\n",
    "from types import SimpleNamespace\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from torch.nn import Linear\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import (MLP, GCNConv, GINConv, global_add_pool,\n",
    "                                global_mean_pool)\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80ed763d-9cb6-4a7b-8dd2-2ea932522c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    print(\"Setting seed:\", seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "def split(input_list, proportions=[0.8, 0.1, 0.1]):\n",
    "    random.shuffle(input_list)\n",
    "    total_length = len(input_list)\n",
    "    lengths = [int(total_length * proportion) for proportion in proportions]\n",
    "\n",
    "    parts = []\n",
    "    start_idx = 0\n",
    "    for length in lengths:\n",
    "        parts.append(input_list[start_idx : start_idx + length])\n",
    "        start_idx += length\n",
    "\n",
    "    return parts\n",
    "\n",
    "\n",
    "def evaluate_model(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            inputs, targets = data.x.to(device), data.y.to(device)\n",
    "            outputs = model(\n",
    "                inputs, data.edge_index.to(device), data.batch.to(device), len(targets)\n",
    "            )\n",
    "            loss = criterion(outputs, targets)\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "\n",
    "    loss = running_loss / len(dataloader)\n",
    "    accuracy = correct / total\n",
    "\n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac044726-cbc2-4b4b-975b-4d84380f0343",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GIN(torch.nn.Module):\n",
    "    \"\"\"https://github.com/pyg-team/pytorch_geometric/blob/master/examples/compile/gin.py\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers):\n",
    "        super().__init__()\n",
    "\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            mlp = MLP([in_channels, hidden_channels, hidden_channels])\n",
    "            self.convs.append(GINConv(nn=mlp, train_eps=False))\n",
    "            in_channels = hidden_channels\n",
    "\n",
    "        self.mlp = MLP(\n",
    "            [hidden_channels, hidden_channels, out_channels], norm=None, dropout=0.5\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index, batch, batch_size):\n",
    "        for conv in self.convs:\n",
    "            x = conv(x, edge_index).relu()\n",
    "        # Pass the batch size to avoid CPU communication/graph breaks:\n",
    "        x = global_add_pool(x, batch, size=batch_size)\n",
    "        return self.mlp(x)\n",
    "\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    \"\"\"https://colab.research.google.com/github/wandb/examples/blob/pyg/graph-classification/colabs/pyg/Graph_Classification_with_PyG_and_W%26B.ipynb\"\"\"\n",
    "\n",
    "    def __init__(self, hidden_channels, inp, out, drop=0.5):\n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.drop = drop\n",
    "        self.conv1 = GCNConv(inp, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.lin = Linear(hidden_channels, out)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # 1. Obtain node embeddings\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv3(x, edge_index)\n",
    "\n",
    "        # 2. Readout layer\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "\n",
    "        # 3. Apply a final classifier\n",
    "        x = F.dropout(x, p=self.drop, training=self.training)\n",
    "        x = self.lin(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "826a587a-6fa6-4fb2-a17b-87a50f685097",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(args):\n",
    "    print(args)\n",
    "\n",
    "    writer = SummaryWriter()\n",
    "    set_seed(args.seed)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    processed_path = os.path.expanduser(\n",
    "        \"~/data/quark-gluon_\"\n",
    "        + str(args.dry_run)\n",
    "        + \"_\"\n",
    "        + str(args.normalize)\n",
    "        + \".pkl\"\n",
    "    )\n",
    "\n",
    "    if os.path.exists(processed_path):\n",
    "        with open(processed_path, \"rb\") as f:\n",
    "            dataset = pickle.load(f)\n",
    "        print(\"Data loaded successfully.\")\n",
    "    else:\n",
    "        with h5py.File(\n",
    "            os.path.expanduser(\"~/data/quark-gluon_data-set_n139306.hdf5\"),\n",
    "            \"r\",\n",
    "        ) as f:\n",
    "            X = np.array(f[\"X_jets\"])\n",
    "            y = np.array(f[\"y\"])\n",
    "            if args.dry_run:\n",
    "                X = X[:40000]\n",
    "                y = y[:40000]\n",
    "\n",
    "        dataset = []\n",
    "        for i, img in tqdm(enumerate(X)):\n",
    "            non_black_pixels = np.where(img.sum(axis=2) > 0)\n",
    "            x_coords, y_coords = non_black_pixels\n",
    "\n",
    "            coords = np.vstack((x_coords, y_coords)).T\n",
    "            node_features = img[x_coords, y_coords]\n",
    "            nbrs = NearestNeighbors(n_neighbors=args.k, algorithm=\"auto\").fit(coords)\n",
    "            _, indices = nbrs.kneighbors(coords)\n",
    "\n",
    "            edge_index = []\n",
    "            for j in range(len(coords)):\n",
    "                for neighbor_idx in indices[j]:\n",
    "                    # if j != neighbor_idx:  # Exclude self-loops\n",
    "                    edge_index.append((j, neighbor_idx))\n",
    "\n",
    "            edge_index = torch.tensor(edge_index, dtype=torch.long).t()\n",
    "\n",
    "            data = Data(\n",
    "                x=torch.tensor(node_features, dtype=torch.float),\n",
    "                edge_index=edge_index,\n",
    "                y=torch.tensor(int(y[i])),\n",
    "            )\n",
    "\n",
    "            if args.normalize:\n",
    "                data = T.NormalizeFeatures()(data)\n",
    "\n",
    "            dataset.append(data)\n",
    "\n",
    "        # Pickle the list to the file\n",
    "        with open(processed_path, \"wb\") as f:\n",
    "            pickle.dump(dataset, f)\n",
    "\n",
    "    if args.dry_run:\n",
    "        dataset = split(dataset, [0.33, 0.33, 0.33])\n",
    "    else:\n",
    "        dataset = split(dataset)\n",
    "\n",
    "    train_loader = DataLoader(dataset[0], batch_size=args.batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(dataset[1], batch_size=args.batch_size, shuffle=False)\n",
    "    val_loader = DataLoader(dataset[2], batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "    model = GIN(\n",
    "        in_channels=3,\n",
    "        hidden_channels=args.hidden,\n",
    "        out_channels=2,\n",
    "        num_layers=args.layers,\n",
    "    ).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), lr=args.lr, weight_decay=args.weight_decay\n",
    "    )\n",
    "    scheduler = StepLR(optimizer, step_size=args.step_size, gamma=args.gamma)\n",
    "\n",
    "    early_stop_thresh = args.step_size * 2\n",
    "    best_accuracy = -1\n",
    "    best_epoch = -1\n",
    "\n",
    "    for epoch in range(args.epochs):\n",
    "        writer.add_scalar(\"epoch\", epoch)\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for data in train_loader:\n",
    "            inputs, targets = data.x.to(device), data.y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(\n",
    "                inputs, data.edge_index.to(device), data.batch.to(device), len(targets)\n",
    "            )\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_acc = correct / total\n",
    "\n",
    "        val_loss, val_acc = evaluate_model(model, val_loader, criterion, device)\n",
    "        writer.add_scalar(\"lr\", optimizer.param_groups[0][\"lr\"])\n",
    "        scheduler.step()\n",
    "        writer.add_scalar(\"train_acc\", train_acc)\n",
    "        writer.add_scalar(\"train_loss\", train_loss)\n",
    "        writer.add_scalar(\"val_acc\", val_acc)\n",
    "        writer.add_scalar(\"val_loss\", val_loss)\n",
    "        print(\n",
    "            f\"Epoch [{epoch + 1}/{args.epochs}], \"\n",
    "            f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n",
    "            f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, \"\n",
    "        )\n",
    "\n",
    "        if val_acc > best_accuracy:\n",
    "            best_accuracy = val_acc\n",
    "            best_epoch = epoch\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"model_state_dict\": model.state_dict()\n",
    "                },\n",
    "                \"best_model.pth\",\n",
    "            )\n",
    "\n",
    "        elif epoch - best_epoch > early_stop_thresh:\n",
    "            print(\"Early stopped training at epoch %d\" % (epoch + 1))\n",
    "            break\n",
    "\n",
    "    checkpoint = torch.load(\"best_model.pth\")\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "    test_loss, test_acc = evaluate_model(model, test_loader, criterion, device)\n",
    "    print(f\"Accuracy: {test_acc:.4f}\")\n",
    "    writer.add_scalar(\"accuracy\", test_acc)\n",
    "    writer.add_scalar(\"loss\", test_loss)\n",
    "    writer.close()\n",
    "    return test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bbb6d74-ce36-4339-948d-8abdb7131016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "namespace(epochs=300, k=2, lr=0.005, batch_size=256, weight_decay=1e-05, hidden=256, dry_run=False, drop=0.5, step_size=10, gamma=0.5, normalize=True, layers=5, seed=42)\n",
      "Setting seed: 42\n",
      "Data loaded successfully.\n",
      "Epoch [1/300], Train Loss: 0.9811, Train Acc: 0.6816, Val Loss: 0.6317, Val Acc: 0.6889, \n",
      "Epoch [2/300], Train Loss: 0.5984, Train Acc: 0.6972, Val Loss: 0.6862, Val Acc: 0.6667, \n",
      "Epoch [3/300], Train Loss: 0.5919, Train Acc: 0.7024, Val Loss: 0.5839, Val Acc: 0.7098, \n",
      "Epoch [4/300], Train Loss: 0.5870, Train Acc: 0.7036, Val Loss: 0.6414, Val Acc: 0.6634, \n",
      "Epoch [5/300], Train Loss: 0.5903, Train Acc: 0.7020, Val Loss: 0.6062, Val Acc: 0.6785, \n",
      "Epoch [6/300], Train Loss: 0.5883, Train Acc: 0.7038, Val Loss: 0.6230, Val Acc: 0.6859, \n",
      "Epoch [7/300], Train Loss: 0.5885, Train Acc: 0.7034, Val Loss: 0.5769, Val Acc: 0.7098, \n",
      "Epoch [8/300], Train Loss: 0.5866, Train Acc: 0.7023, Val Loss: 0.5828, Val Acc: 0.7028, \n",
      "Epoch [9/300], Train Loss: 0.5846, Train Acc: 0.7062, Val Loss: 0.5947, Val Acc: 0.6875, \n",
      "Epoch [10/300], Train Loss: 0.5818, Train Acc: 0.7063, Val Loss: 0.5892, Val Acc: 0.6984, \n",
      "Epoch [11/300], Train Loss: 0.5773, Train Acc: 0.7102, Val Loss: 0.5794, Val Acc: 0.7035, \n",
      "Epoch [12/300], Train Loss: 0.5779, Train Acc: 0.7110, Val Loss: 0.5711, Val Acc: 0.7164, \n",
      "Epoch [13/300], Train Loss: 0.5776, Train Acc: 0.7098, Val Loss: 0.5946, Val Acc: 0.6942, \n",
      "Epoch [14/300], Train Loss: 0.5757, Train Acc: 0.7114, Val Loss: 0.9035, Val Acc: 0.5338, \n",
      "Epoch [15/300], Train Loss: 0.5773, Train Acc: 0.7101, Val Loss: 0.5671, Val Acc: 0.7182, \n",
      "Epoch [16/300], Train Loss: 0.5763, Train Acc: 0.7103, Val Loss: 0.6627, Val Acc: 0.6753, \n",
      "Epoch [17/300], Train Loss: 0.5778, Train Acc: 0.7108, Val Loss: 0.5711, Val Acc: 0.7130, \n",
      "Epoch [18/300], Train Loss: 0.5780, Train Acc: 0.7115, Val Loss: 0.5697, Val Acc: 0.7136, \n",
      "Epoch [19/300], Train Loss: 0.5765, Train Acc: 0.7111, Val Loss: 0.5868, Val Acc: 0.6965, \n",
      "Epoch [20/300], Train Loss: 0.5748, Train Acc: 0.7121, Val Loss: 0.5702, Val Acc: 0.7171, \n",
      "Epoch [21/300], Train Loss: 0.5719, Train Acc: 0.7146, Val Loss: 0.5707, Val Acc: 0.7131, \n",
      "Epoch [22/300], Train Loss: 0.5724, Train Acc: 0.7141, Val Loss: 0.5638, Val Acc: 0.7186, \n",
      "Epoch [23/300], Train Loss: 0.5707, Train Acc: 0.7152, Val Loss: 0.5994, Val Acc: 0.6848, \n",
      "Epoch [24/300], Train Loss: 0.5709, Train Acc: 0.7146, Val Loss: 0.5902, Val Acc: 0.6925, \n",
      "Epoch [25/300], Train Loss: 0.5712, Train Acc: 0.7132, Val Loss: 0.6296, Val Acc: 0.6679, \n",
      "Epoch [26/300], Train Loss: 0.5709, Train Acc: 0.7150, Val Loss: 0.5693, Val Acc: 0.7137, \n",
      "Epoch [27/300], Train Loss: 0.5703, Train Acc: 0.7153, Val Loss: 0.5681, Val Acc: 0.7170, \n",
      "Epoch [28/300], Train Loss: 0.5711, Train Acc: 0.7145, Val Loss: 0.5683, Val Acc: 0.7153, \n",
      "Epoch [29/300], Train Loss: 0.5702, Train Acc: 0.7139, Val Loss: 0.5755, Val Acc: 0.7062, \n",
      "Epoch [30/300], Train Loss: 0.5703, Train Acc: 0.7157, Val Loss: 0.5650, Val Acc: 0.7154, \n",
      "Epoch [31/300], Train Loss: 0.5679, Train Acc: 0.7165, Val Loss: 0.6139, Val Acc: 0.6753, \n",
      "Epoch [32/300], Train Loss: 0.5683, Train Acc: 0.7172, Val Loss: 0.5648, Val Acc: 0.7176, \n",
      "Epoch [33/300], Train Loss: 0.5676, Train Acc: 0.7155, Val Loss: 0.5737, Val Acc: 0.7105, \n",
      "Epoch [34/300], Train Loss: 0.5671, Train Acc: 0.7170, Val Loss: 0.5677, Val Acc: 0.7173, \n",
      "Epoch [35/300], Train Loss: 0.5671, Train Acc: 0.7169, Val Loss: 0.5730, Val Acc: 0.7125, \n",
      "Epoch [36/300], Train Loss: 0.5668, Train Acc: 0.7170, Val Loss: 0.5625, Val Acc: 0.7182, \n",
      "Epoch [37/300], Train Loss: 0.5677, Train Acc: 0.7166, Val Loss: 0.5641, Val Acc: 0.7181, \n",
      "Epoch [38/300], Train Loss: 0.5669, Train Acc: 0.7173, Val Loss: 0.5629, Val Acc: 0.7217, \n",
      "Epoch [39/300], Train Loss: 0.5658, Train Acc: 0.7190, Val Loss: 0.5719, Val Acc: 0.7128, \n",
      "Epoch [40/300], Train Loss: 0.5658, Train Acc: 0.7185, Val Loss: 0.5633, Val Acc: 0.7200, \n",
      "Epoch [41/300], Train Loss: 0.5650, Train Acc: 0.7186, Val Loss: 0.5603, Val Acc: 0.7232, \n",
      "Epoch [42/300], Train Loss: 0.5648, Train Acc: 0.7196, Val Loss: 0.5598, Val Acc: 0.7219, \n",
      "Epoch [43/300], Train Loss: 0.5642, Train Acc: 0.7187, Val Loss: 0.5619, Val Acc: 0.7212, \n",
      "Epoch [44/300], Train Loss: 0.5643, Train Acc: 0.7203, Val Loss: 0.5662, Val Acc: 0.7127, \n",
      "Epoch [45/300], Train Loss: 0.5635, Train Acc: 0.7196, Val Loss: 0.5753, Val Acc: 0.7108, \n",
      "Epoch [46/300], Train Loss: 0.5634, Train Acc: 0.7202, Val Loss: 0.5628, Val Acc: 0.7211, \n",
      "Epoch [47/300], Train Loss: 0.5644, Train Acc: 0.7190, Val Loss: 0.5597, Val Acc: 0.7220, \n",
      "Epoch [48/300], Train Loss: 0.5640, Train Acc: 0.7195, Val Loss: 0.5597, Val Acc: 0.7206, \n",
      "Epoch [49/300], Train Loss: 0.5632, Train Acc: 0.7198, Val Loss: 0.5594, Val Acc: 0.7229, \n",
      "Epoch [50/300], Train Loss: 0.5638, Train Acc: 0.7202, Val Loss: 0.5715, Val Acc: 0.7101, \n",
      "Epoch [51/300], Train Loss: 0.5628, Train Acc: 0.7196, Val Loss: 0.5598, Val Acc: 0.7211, \n",
      "Epoch [52/300], Train Loss: 0.5622, Train Acc: 0.7204, Val Loss: 0.5597, Val Acc: 0.7210, \n",
      "Epoch [53/300], Train Loss: 0.5621, Train Acc: 0.7210, Val Loss: 0.5594, Val Acc: 0.7232, \n",
      "Epoch [54/300], Train Loss: 0.5626, Train Acc: 0.7202, Val Loss: 0.5586, Val Acc: 0.7218, \n",
      "Epoch [55/300], Train Loss: 0.5623, Train Acc: 0.7204, Val Loss: 0.5602, Val Acc: 0.7206, \n",
      "Epoch [56/300], Train Loss: 0.5624, Train Acc: 0.7209, Val Loss: 0.5589, Val Acc: 0.7223, \n",
      "Epoch [57/300], Train Loss: 0.5624, Train Acc: 0.7205, Val Loss: 0.5605, Val Acc: 0.7225, \n",
      "Epoch [58/300], Train Loss: 0.5625, Train Acc: 0.7206, Val Loss: 0.5669, Val Acc: 0.7153, \n",
      "Epoch [59/300], Train Loss: 0.5615, Train Acc: 0.7200, Val Loss: 0.5604, Val Acc: 0.7218, \n",
      "Epoch [60/300], Train Loss: 0.5619, Train Acc: 0.7212, Val Loss: 0.5648, Val Acc: 0.7187, \n",
      "Epoch [61/300], Train Loss: 0.5613, Train Acc: 0.7208, Val Loss: 0.5589, Val Acc: 0.7211, \n",
      "Epoch [62/300], Train Loss: 0.5605, Train Acc: 0.7216, Val Loss: 0.5590, Val Acc: 0.7225, \n",
      "Early stopped training at epoch 62\n",
      "Accuracy: 0.7255\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7254845656855707"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = SimpleNamespace(\n",
    "        epochs=300,\n",
    "        k=2,\n",
    "        lr=0.005,\n",
    "        batch_size=256,\n",
    "        weight_decay=1.0e-05,\n",
    "        hidden=256,\n",
    "        dry_run=False,\n",
    "        drop=0.5,\n",
    "        step_size=10,\n",
    "        gamma=0.5,\n",
    "        normalize=True,\n",
    "        layers=5,\n",
    "        seed=42\n",
    "    )\n",
    "run(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed51423-ae60-4ad6-b68c-dfb714686fdf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
